{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, we import the required libraries to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import Laplace\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the text file and put it in the text variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hp_en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_en = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hp_fa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_fa = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part a) Applying the necessary pre-processings:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the text by using the \"hazm\" library, which is for processing Persian texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = hazm.Normalizer()\n",
    "normalized_text = normalizer.normalize(text_fa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of this library, we segment the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7021\n"
     ]
    }
   ],
   "source": [
    "sentences = hazm.sent_tokenize(normalized_text)\n",
    "print(\"Number of sentences: \" + str(len(sentences)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we segment the words and take them out for processing in the form shown in the library guide (https://www.nltk.org/api/nltk.lm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    new_sentences.append(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the sentences, we add characters for the beginning and end of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x2c883ac2590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(pad_both_ends(sent, n=2) for sent in new_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part b) Training language model:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams of first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '\\ufeffآقا'), ('\\ufeffآقا', 'و'), ('و', 'خانم'), ('خانم', 'دورسلی'), ('دورسلی', 'ساکن'), ('ساکن', 'خانه'), ('خانه', 'شماره'), ('شماره', 'چهار'), ('چهار', 'خیابان'), ('خیابان', 'پریوت'), ('پریوت', 'درایو'), ('درایو', 'بودند'), ('بودند', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigrams(pad_both_ends(new_sentences[0], n=2))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default preprocessing for a sequence of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2, vocab_2 = padded_everygram_pipeline(2, new_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define model with add-1 laplace smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_2 = Laplace(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train model on our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9467\n"
     ]
    }
   ],
   "source": [
    "lm_2.fit(train_2, vocab_2)\n",
    "print(len(lm_2.vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of 2-grams in the text is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 212119 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we want to know what is the chance that “هوا” is preceded by “امسال”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010552975939214858\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.score(\"هوا\", [\"امسال\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part c) Importance of Laplace smoothing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing statistical analysis on a dataset that contains categorical variables, it is possible that some categories in the dataset may have zero frequency, which can lead to issues with statistical models that assume non-zero probabilities for all categories.\n",
    "\n",
    "Without Laplace smoothing, if a category in the dataset has zero frequency, then the probability of that category occurring will be zero as well.\n",
    "\n",
    "Laplace smoothing helps to avoid this problem by adding a small amount of probability mass to all categories, even those with zero frequency. This ensures that all categories have non-zero probabilities, and it helps to prevent overfitting to the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part d) Generating new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['یادمون', 'رفت', 'تعقیبش', 'کنه', 'که', 'وقت', 'چنین', 'استنباط', 'کرد', 'و', 'غول', 'پیکر', 'که', 'جادوگر', 'بوده', '.', '</s>', 'را', 'به', 'طبقه\\u200cی', 'بالا', 'گرفت', 'واقعیت']\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', '-', 'هیس', '!', '</s>', 'اتومبیلی', 'را', 'شناخت', 'و', 'منتظر', 'چیست', '.', '</s>', '…']\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['دست', 'چاقش', 'که', 'از', 'ساعت', 'یازده', 'سیکل', 'نقره', 'ایه', '.', '</s>', 'چیزی', 'میدانست', 'کجا', 'فهمیدین', 'ولی', 'بعد', 'متوجه', 'حالت', 'رئیس', 'این', 'قدر']\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['تونسته', 'راه', 'می\\u200cرفت', 'و', 'سر', 'داد', '.', '</s>', 'ما', 'توی', 'پاتیل', 'درزدار', 'برد', 'دوباره', 'جام', 'قهرمانی', 'صعود']\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['همه', 'دوستشون', 'دارن', 'بمبارون', 'میکنن؟', '</s>', 'خوردن', 'شیرینی', 'تعارف', 'کرد', '.', '</s>', '</s>', '<s>', 'توی', 'اسلیترین', 'میتونه', 'جلوتونو', 'بگیره', '.', '</s>', 'و', 'احمق', '!']\n"
     ]
    }
   ],
   "source": [
    "print(lm_2.generate(random.randint(12, 24)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part e) 3-grams and 5-grams:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3-gram we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3, vocab_3 = padded_everygram_pipeline(3, new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_3 = Laplace(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9467\n"
     ]
    }
   ],
   "source": [
    "lm_3.fit(train_3, vocab_3)\n",
    "print(len(lm_3.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['را', 'به', 'چنگ', 'آورد', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_3.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_3.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['باید', 'به', 'بدعنق', 'فرصت', 'بدیم؟', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_3.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_3.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['همیشه', 'اعضای', 'تیمش', 'را', 'به', 'سوی', 'آن\\u200cها', 'می\\u200cآمد', 'پوزخندی', 'زد', 'و', 'به', 'سوی', 'آشپزخانه', 'رفت', 'و', 'پچ', 'پچ', 'ضعیفی', 'را']\n"
     ]
    }
   ],
   "source": [
    "print(lm_3.generate(random.randint(12, 24)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 5-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_5, vocab_5 = padded_everygram_pipeline(5, new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_5 = Laplace(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9467\n"
     ]
    }
   ],
   "source": [
    "lm_5.fit(train_5, vocab_5)\n",
    "print(len(lm_5.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<s>', '-', 'تو', 'نباید', 'شب', 'توی', 'مدرسه', 'پرسه', 'بزنی', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_5.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_5.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['برداشت', 'کنیم', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_5.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وقتی', 'آن', 'موجود', 'در', 'نور', 'مهتاب', 'قرار', 'گرفت', 'آن', 'را', 'ببینند', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_5.generate(random.randint(12, 24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm_5.generate(random.randint(12, 24)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part f) Comparing sentencs that are generated by 3 models:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gram model is simpler and faster to compute, but it can not capture longer-range dependencies between words in a text.\n",
    "\n",
    "5-gram model is more complex and computationally expensive, but it may be able to capture more complex relationships between words in a text. However, it can not generate good sentences when vocab size is not big enough (case of this problem).\n",
    "\n",
    "#### 3-gram model can capture some longer-range dependencies between words, and make a good trade-of between 2-gram and 5-gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part g) Perplexity of model:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the content of the first sentence is similar to the content on which the model was trained and the difference in the style of the second sentence with the text on which the model was trained, the probability of the occurrence of the first sentence in the text is higher than the second sentence. Therefore, the perplexity of the second sentence will be more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tabulate import tabulate\n",
    "from tokenizers.decoders import ByteLevel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part a) Three tokenization methods:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The white space tokenizer simply splits text into tokens based on white space characters like spaces, tabs, and newlines.\n",
    "\n",
    "- spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token.\n",
    "\n",
    "- Byte Pair Encoding (BPE) is a type of subword tokenizer that learns to represent words as a sequence of smaller subword units. This can be useful for handling rare or out-of-vocabulary words, as well as for languages with complex morphological structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part b) Text tokenization:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White space tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1_en = text_en.split()\n",
    "tokens_1_fa = text_fa.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!-m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc_en = nlp(text_en)\n",
    "doc_fa = nlp(text_fa)\n",
    "\n",
    "tokens_2_en = [token.text for token in doc_en]\n",
    "tokens_2_fa = [token.text for token in doc_fa]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer_en = ByteLevelBPETokenizer()\n",
    "bpe_tokenizer_fa = ByteLevelBPETokenizer()\n",
    "\n",
    "\n",
    "bpe_tokenizer_en.train_from_iterator([text_en])\n",
    "bpe_tokenizer_fa.train_from_iterator([normalized_text])\n",
    "\n",
    "\n",
    "bpe_encoded_en = bpe_tokenizer_en.encode(text_en)\n",
    "bpe_encoded_fa = bpe_tokenizer_fa.encode(normalized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 'tabulate' library for drawing the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════╤═════════════════════════════════════╤═════════════════════════════════════╕\n",
      "│ Algorithm   │   Number of Tokens for Persian Book │   Number of Tokens for English Book │\n",
      "╞═════════════╪═════════════════════════════════════╪═════════════════════════════════════╡\n",
      "│ White Space │                               96294 │                               78443 │\n",
      "├─────────────┼─────────────────────────────────────┼─────────────────────────────────────┤\n",
      "│ spacy       │                              124822 │                              102406 │\n",
      "├─────────────┼─────────────────────────────────────┼─────────────────────────────────────┤\n",
      "│ BPE         │                              116168 │                              107701 │\n",
      "╘═════════════╧═════════════════════════════════════╧═════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "data = [[\"White Space\", len(tokens_1_fa), len(tokens_1_en)], \n",
    "        [\"spacy\", len(tokens_2_fa), len(tokens_2_en)], \n",
    "        [\"BPE\", len(bpe_encoded_fa), len(bpe_encoded_en)]]\n",
    "col_names = [\"Algorithm\", \"Number of Tokens for Persian Book\", \"Number of Tokens for English Book\"]\n",
    "\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part c) Tokenizing test inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_input = 'This question is about tokenization and shows several tokenizer algorithms.Hopefully, you will be able to understand how they are trained and generate tokens.'\n",
    "fa_input = 'این سوال در مورد قطعه بندی جملات است و چندین الگوریتم توکنایز کردن متن را نشان می دهد. امیدواریم بتوانید نحوه آموزش آنها و تولید توکن ها را درک کنید.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing inputs with white space tokenization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'question', 'is', 'about', 'tokenization', 'and', 'shows', 'several', 'tokenizer', 'algorithms.Hopefully,', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens.']\n",
      "['این', 'سوال', 'در', 'مورد', 'قطعه', 'بندی', 'جملات', 'است', 'و', 'چندین', 'الگوریتم', 'توکنایز', 'کردن', 'متن', 'را', 'نشان', 'می', 'دهد.', 'امیدواریم', 'بتوانید', 'نحوه', 'آموزش', 'آنها', 'و', 'تولید', 'توکن', 'ها', 'را', 'درک', 'کنید.']\n"
     ]
    }
   ],
   "source": [
    "test_tokens_en_1 = en_input.split()\n",
    "test_tokens_fa_1 = fa_input.split()\n",
    "\n",
    "print(test_tokens_en_1)\n",
    "print(test_tokens_fa_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing inputs with spacy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'question', 'is', 'about', 'tokenization', 'and', 'shows', 'several', 'tokenizer', 'algorithms', '.', 'Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n",
      "['این', 'سوال', 'در', 'مورد', 'قطعه', 'بندی', 'جملات', 'است', 'و', 'چندین', 'الگوریتم', 'توکنایز', 'کردن', 'متن', 'را', 'نشان', 'می', 'دهد', '.', 'امیدواریم', 'بتوانید', 'نحوه', 'آموزش', 'آنها', 'و', 'تولید', 'توکن', 'ها', 'را', 'درک', 'کنید', '.']\n"
     ]
    }
   ],
   "source": [
    "test_en = nlp(en_input)\n",
    "test_fa = nlp(fa_input)\n",
    "\n",
    "test_tokens_en_2 = [token.text for token in test_en]\n",
    "test_tokens_fa_2 = [token.text for token in test_fa]\n",
    "\n",
    "print(test_tokens_en_2)\n",
    "print(test_tokens_fa_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing inputs with BPE tokenization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġquestion', 'Ġis', 'Ġabout', 'Ġto', 'ken', 'iz', 'ation', 'Ġand', 'Ġshows', 'Ġseveral', 'Ġto', 'ken', 'iz', 'er', 'Ġal', 'g', 'or', 'ith', 'm', 's', '.', 'Hope', 'fully', ',', 'Ġyou', 'Ġwill', 'Ġbe', 'Ġable', 'Ġto', 'Ġunderstand', 'Ġhow', 'Ġthey', 'Ġare', 'Ġtrain', 'ed', 'Ġand', 'Ġg', 'en', 'er', 'ate', 'Ġto', 'ken', 's', '.']\n",
      "['این', ' سو', 'ال', ' در', ' مورد', ' قط', 'عه', ' بندی', ' جم', 'لات', ' است', ' و', ' چندین', ' ال', 'گ', 'وری', 'تم', ' تو', 'کن', 'ای', 'ز', ' کردن', ' متن', ' را', ' نشان', ' می', ' دهد', '.', ' امید', 'و', 'اریم', ' ب', 'توان', 'ید', ' نح', 'وه', ' آموزش', ' آنها', ' و', ' تو', 'ل', 'ید', ' تو', 'کن', ' ها', ' را', ' درک', ' کن', 'ید', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding_en = bpe_tokenizer_en.encode(en_input)\n",
    "encoding_fa = bpe_tokenizer_fa.encode(fa_input)\n",
    "\n",
    "#Resolving the problem of BPE with utf-8 characters:\n",
    "decoder = ByteLevel()\n",
    "repaired = [decoder.decode([elem]) for elem in encoding_fa.tokens]\n",
    "\n",
    "print(encoding_en.tokens)\n",
    "print(repaired)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of BPE allows us to separate the words obtained from the combination of several small parts into sub-words. For example, the word \"tokenization\" is divided into three sub-words: \"to\", \"ken\", \"iz\", and \"ation\". Due to the fact that BPE is based on machine learning and is taught with the help of text, it also works well in Persian language.\n",
    "\n",
    "The main problem with white space tokenization is that words are mistakenly treated as a token when they are not separated by spaces. For example, this happened at the end of the first English sentence. ('algorithms.Hopefully,')\n",
    "\n",
    "Although spacy algoriyhm cannot recognize sub-words, it has a better performance in recognizing words that are not separated by spaces and can also recognize puctuations. However, this algorithm does not perform well in Persian language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
